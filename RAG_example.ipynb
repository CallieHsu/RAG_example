{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# fix chroma version error\n",
    "%pip install -q \"pysqlite3-binary\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# for UnstructuredMarkdownLoader\n",
    "%pip install -q \"unstructured\"\n",
    "%pip install -q \"markdown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import (CSVLoader, UnstructuredMarkdownLoader, TextLoader)\n",
    "\n",
    "# fix chroma version error\n",
    "__import__('pysqlite3')\n",
    "import pysqlite3\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "### import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyMuPDFLoader(\"Virtual_characters.pdf\")\n",
    "# PDF_data = loader.load()\n",
    "\n",
    "# loader = CSVLoader(\"shuttle-XPC-cube-less.csv\")\n",
    "# CSV_data = loader.load()\n",
    "# print(CSV_data)\n",
    "\n",
    "# loader = UnstructuredMarkdownLoader(\"shuttle-xpc-cube.md\")\n",
    "# MD_data = loader.load()\n",
    "\n",
    "loader = TextLoader(\"shuttle-cube-usa.txt\", encoding=\"utf8\")\n",
    "TXT_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "# all_splits = text_splitter.split_documents(PDF_data)\n",
    "all_splits = text_splitter.split_documents(TXT_data)\n",
    "# print(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_gpu_layers=100,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='<<SYS>> \\n    You are a helpful assistant eager to assist with providing better Google search results.\\n    <</SYS>> \\n    \\n    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative,             relevant, and concise:\\n            {question} \\n    [/INST]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \n",
    "    You are a helpful assistant eager to assist with providing better Google search results.\n",
    "    <</SYS>> \n",
    "    \n",
    "    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative, \\\n",
    "            relevant, and concise:\n",
    "            {question} \n",
    "    [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are a helpful assistant eager to assist with providing better Google search results. \\\n",
    "        Provide an answer to the following question in about 150 words. Ensure that the answer is informative, \\\n",
    "        relevant, and concise: \\\n",
    "        {question}\"\"\",\n",
    ")\n",
    "\n",
    "# DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"\"\"<<SYS>> \n",
    "#     You are a helpful assistant eager to assist with providing better Google search results.\n",
    "#     <</SYS>> \n",
    "    \n",
    "#     [INST] Use the following pieces of retrieved context to answer the question. If you don't know the answer, \\\n",
    "#             just say that you don't know. Use three sentences maximum and keep the answer concise:\n",
    "#             {question} \n",
    "#     [/INST]\"\"\",\n",
    "# )\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# question = \"What is Taiwan known for?\"\n",
    "# llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The Shuttle XPC cube series consists of two models, SH610R4 and SH310R4 V2. Both models have similar dimensions, power supply input, and expansion slots but differ in their power supply output and additional accessories provided. The SH610R4 has a 80 Plus 300W power supply input, while the SH310R4 V2 has a 80 Plus 500W power supply input. Additionally, the SH610R4 comes with a DVD driver and SATA cable x4 (For HDD), while the SH310R4 V2 comes with XPC Multi-Language quick guide and CPU thermal grease."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =      49.50 ms /   154 runs   (    0.32 ms per token,  3110.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29462.52 ms /   608 tokens (   48.46 ms per token,    20.64 tokens per second)\n",
      "llama_print_timings:        eval time =   38003.65 ms /   153 runs   (  248.39 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:       total time =   67922.12 ms /   761 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The SH610R4 supports Intel® 12th/13th generation Core™ i3/i5/i7/i9, Pentium®, and Celeron® processors with a maximum power consumption of 125W and a frequency range of 2933/3200 MHz."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =      25.80 ms /    74 runs   (    0.35 ms per token,  2868.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72455.91 ms /  1506 tokens (   48.11 ms per token,    20.79 tokens per second)\n",
      "llama_print_timings:        eval time =   20471.62 ms /    73 runs   (  280.43 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =   93151.94 ms /  1579 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The processor of SZ270R9 is a 7th Gen Intel® Core™ i7 processor supporting Kabylake/Skylake architecture with a TDP of 95W and compatible with LGA 1151 socket. It offers optimal gaming and VR performance with a maximum clock speed of up to 4.5 GHz."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =      28.26 ms /    80 runs   (    0.35 ms per token,  2831.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   58351.31 ms /  1257 tokens (   46.42 ms per token,    21.54 tokens per second)\n",
      "llama_print_timings:        eval time =   21799.91 ms /    79 runs   (  275.95 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =   80394.43 ms /  1336 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The chipset of SH570R8 is Intel H570 express chipset."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =       7.69 ms /    23 runs   (    0.33 ms per token,  2992.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3661.74 ms /    81 tokens (   45.21 ms per token,    22.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5163.80 ms /    22 runs   (  234.72 ms per token,     4.26 tokens per second)\n",
      "llama_print_timings:       total time =    8888.77 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The SH510R4 supports Intel Core processors in the LGA1200 socket (Gen 10/11)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =      10.43 ms /    31 runs   (    0.34 ms per token,  2972.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74137.52 ms /  1534 tokens (   48.33 ms per token,    20.69 tokens per second)\n",
      "llama_print_timings:        eval time =    8394.14 ms /    30 runs   (  279.80 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =   82625.61 ms /  1564 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The dimensions of SW580R8 are 332(L) x 216(W) x 198(H) mm."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =      12.27 ms /    36 runs   (    0.34 ms per token,  2933.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6397.80 ms /   147 tokens (   43.52 ms per token,    22.98 tokens per second)\n",
      "llama_print_timings:        eval time =    8191.22 ms /    35 runs   (  234.03 ms per token,     4.27 tokens per second)\n",
      "llama_print_timings:       total time =   14689.92 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The processor supported by SW580R8 is Intel processors in the LGA1200 socket (Gen 10/11) & Xeon-W series.\n",
      "Unhelpful Answer: The processor is blue and has 4 cores."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   24495.80 ms\n",
      "llama_print_timings:      sample time =      19.10 ms /    57 runs   (    0.34 ms per token,  2984.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   83453.73 ms /  1714 tokens (   48.69 ms per token,    20.54 tokens per second)\n",
      "llama_print_timings:        eval time =   15836.97 ms /    56 runs   (  282.80 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:       total time =   99468.08 ms /  1770 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about Alison Hawk's career and age\"\n",
    "query = \"Tell me about the Shuttle XPC cube series and its models.\"\n",
    "query = \"Tell me about the processor of SH610R4\"\n",
    "\n",
    "querys = [\n",
    "    \"Tell me about the Shuttle XPC cube series and its models.\",\n",
    "    \"Tell me about the processor of SH610R4.\",\n",
    "    \"Tell me about the processor of SZ270R9.\",\n",
    "    \"What is the chipset of SH570R8?\",\n",
    "    \"What processor used in SH510R4?\",\n",
    "    \"What is the dimensions of SW580R8?\",\n",
    "    \"Tell me about the processor of SW580R8.\"\n",
    "]\n",
    "\n",
    "# qa.invoke(querys[-1])\n",
    "\n",
    "for query in querys:\n",
    "    qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "## fix chroma version error\n",
    "__import__('pysqlite3')\n",
    "import pysqlite3\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing documents\n",
    "doc_path = \"doc_shuttle\"\n",
    "loader = DirectoryLoader(doc_path, glob='**/*.txt', loader_cls=TextLoader)\n",
    "TXT_data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(TXT_data)\n",
    "# print(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embed and store the texts\n",
    "## Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    13.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    n_gpu_layers=100,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='<<SYS>> \\n    You are a helpful assistant eager to assist with providing better Google search results.\\n    <</SYS>> \\n    \\n    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative,             relevant, and concise:\\n            {question} \\n    [/INST]')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<|system|>\n",
    "    You are a helpful assistant eager to assist with providing better Google search results.\n",
    "    Provide an answer to the following question in 150 words. Ensure that the answer is informative, \\\n",
    "    relevant, and concise:\n",
    "            {question} </s>\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_TINY_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \n",
    "    You are a helpful assistant eager to assist with providing better Google search results.\n",
    "    <</SYS>> \n",
    "    \n",
    "    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative, \\\n",
    "            relevant, and concise:\n",
    "            {question} \n",
    "    [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are a helpful assistant eager to assist with providing better Google search results. \\\n",
    "        Provide an answer to the following question in about 150 words. Ensure that the answer is informative, \\\n",
    "        relevant, and concise: \\\n",
    "        {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# question = \"What is Taiwan known for?\"\n",
    "# llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The Shuttle XPC cube series is a line of compact and powerful PCs designed for various applications. The series includes several models with different specifications and features. Here are some key details about the Shuttle XPC cube series and its models:\n",
      "The Shuttle XPC cube series includes the following models:\n",
      "* Shuttle XPC cube 100: This is the base model with an Intel 10th Gen processor, 8GB RAM, and a 256GB SSD. It has a 16x PCIe Gen4 slot and supports Linux and Windows 10 OS.\n",
      "* Shuttle XPC cube 200: This model has a higher specification than the 100 series with Intel 11th Gen processor, 16GB RAM, and a 512GB SSD. It also has a 16x PCIe Gen4 slot and supports Linux and Windows 10 OS.\n",
      "* Shuttle XPC cube 300: This model has a higher specification than the 200 series with Intel 11th Gen processor, 16GB RAM, and a 1TB SSD."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =      90.38 ms /   256 runs   (    0.35 ms per token,  2832.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73652.67 ms /  1536 tokens (   47.95 ms per token,    20.85 tokens per second)\n",
      "llama_print_timings:        eval time =   74619.53 ms /   256 runs   (  291.48 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =  149113.63 ms /  1792 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The processor of SH610R4 is supported by Intel 12th/13th generation Core i3/i5/i7/i9, Pentium, and Celeron 125W LGA1700 CPU."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =      19.04 ms /    56 runs   (    0.34 ms per token,  2941.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73204.01 ms /  1525 tokens (   48.00 ms per token,    20.83 tokens per second)\n",
      "llama_print_timings:        eval time =   15926.26 ms /    55 runs   (  289.57 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =   89311.15 ms /  1580 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The SZ270R9 supports Kabylake/Skylake 95W Core i3/i5/i7 LGA 1151 CPU processor.\n",
      "Unhelpful Answer: The SZ270R9 has a processor that is made of chocolate and has rainbows coming out of it."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =      28.07 ms /    76 runs   (    0.37 ms per token,  2707.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   78954.16 ms /  1604 tokens (   49.22 ms per token,    20.32 tokens per second)\n",
      "llama_print_timings:        eval time =   23381.10 ms /    75 runs   (  311.75 ms per token,     3.21 tokens per second)\n",
      "llama_print_timings:       total time =  102597.58 ms /  1679 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The chipset of SH570R8 is Intel H570 express chipset."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =       8.61 ms /    23 runs   (    0.37 ms per token,  2672.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   78909.04 ms /  1585 tokens (   49.78 ms per token,    20.09 tokens per second)\n",
      "llama_print_timings:        eval time =    6555.47 ms /    22 runs   (  297.98 ms per token,     3.36 tokens per second)\n",
      "llama_print_timings:       total time =   85553.16 ms /  1607 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The processor used in SH510R4 is Intel Core processors of the 10th and 11th generations for LGA1200 sockets."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =      13.36 ms /    39 runs   (    0.34 ms per token,  2919.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82642.02 ms /  1668 tokens (   49.55 ms per token,    20.18 tokens per second)\n",
      "llama_print_timings:        eval time =   11728.53 ms /    38 runs   (  308.65 ms per token,     3.24 tokens per second)\n",
      "llama_print_timings:       total time =   94516.11 ms /  1706 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The dimensions of the SW580R8 micro server are 13 inches (L) × 8.5 inches (W) × 7.5 inches (H)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =      12.91 ms /    39 runs   (    0.33 ms per token,  3021.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   71885.61 ms /  1472 tokens (   48.84 ms per token,    20.48 tokens per second)\n",
      "llama_print_timings:        eval time =   10743.22 ms /    38 runs   (  282.72 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:       total time =   82754.56 ms /  1510 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " The processor supported by SW580R8 is Intel 10th/11th generation Core i3/i5/i7, Pentium, and Celeron 125W LGA1200 CPU."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   23336.51 ms\n",
      "llama_print_timings:      sample time =      19.12 ms /    52 runs   (    0.37 ms per token,  2719.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22345.39 ms /   424 tokens (   52.70 ms per token,    18.97 tokens per second)\n",
      "llama_print_timings:        eval time =   15648.42 ms /    51 runs   (  306.83 ms per token,     3.26 tokens per second)\n",
      "llama_print_timings:       total time =   38193.62 ms /   475 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# query = \"Tell me about the processor of SH610R4\"\n",
    "# qa.invoke(query)\n",
    "\n",
    "querys = [\n",
    "    \"Tell me about the Shuttle XPC cube series and its models.\",\n",
    "    \"Tell me about the processor of SH610R4.\",\n",
    "    \"Tell me about the processor of SZ270R9.\",\n",
    "    \"What is the chipset of SH570R8?\",\n",
    "    \"What processor used in SH510R4?\",\n",
    "    \"What is the dimensions of SW580R8?\",\n",
    "    \"Tell me about the processor of SW580R8.\"\n",
    "]\n",
    "\n",
    "for query in querys:\n",
    "    qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
